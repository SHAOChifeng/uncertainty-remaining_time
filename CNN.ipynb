{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-methodology",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticCNN_1D(nn.Module):\n",
    "    def __init__(self, emb_dims=None, hidden_size=1024,\n",
    "                 weight_regularizer=.1, dropout_regularizer=.1,\n",
    "                 conv_dropout_traditional=False, dropout_dense_only=False,\n",
    "                 input_size=1, c=1, hs=True, kernel_size=3, nr_kernels = 20,\n",
    "                 nr_conv_layers=2, seq_len=6,\n",
    "                 dropout=True, concrete=True, p_fix=0.01, Bayes=True):\n",
    "\n",
    "        '''\n",
    "        ARGUMENTS:\n",
    "        emb_dims: list of tuples (a, b) for each categorical variable,\n",
    "                  with a: number of levels, and b: embedding dimension\n",
    "        hidden_size: number of nodes in dense layers\n",
    "        weight_regularizer: parameter for weight regularization in reformulated ELBO\n",
    "        dropout_regularizer: parameter for dropout regularization in reformulated ELBO\n",
    "        conv_dropout_traditional: if \"True\" then traditional dropout between convolutional layers\n",
    "        dropout_dense_only: if \"True\" then only dropout in dense layers, not in convolutional layers\n",
    "        input_size: number of range features\n",
    "        c: number of outputs (one for remaining time prediction)\n",
    "        hs: \"True\" if heteroscedastic, \"False\" if homoscedastic\n",
    "        kernel_size: size of kernels in convolutional layers\n",
    "        nr_kernels: number of kernels in convolutional layers\n",
    "        nr_conv_layers: number of convolutional layers\n",
    "        seq_len: sequence length\n",
    "        dropout: in case of deterministic model, apply dropout if \"True\", otherwise no dropout\n",
    "        concrete: dropout parameter is fixed when \"False\". If \"True\", then concrete dropout\n",
    "        p_fix: dropout parameter in case \"concrete\"=\"False\"\n",
    "        Bayes: BNN if \"True\", deterministic model if \"False\" (only sampled once for inference)\n",
    "        '''\n",
    "\n",
    "        self.heteroscedastic = hs\n",
    "        self.nr_conv_layers = nr_conv_layers\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.no_of_embs = 0\n",
    "        if emb_dims:\n",
    "            self.emb_layers = nn.ModuleList([nn.Embedding(x, y)\n",
    "                                             for x, y in emb_dims])\n",
    "            self.no_of_embs = sum([y for x, y in emb_dims])\n",
    "\n",
    "        self.conv = \"1D\"\n",
    "        if conv_dropout_traditional:\n",
    "            self.conv = \"lin\"\n",
    "\n",
    "        dropout_conv = dropout\n",
    "        if dropout_dense_only:\n",
    "            dropout_conv = False\n",
    "\n",
    "        len_conv1 = self.no_of_embs + input_size\n",
    "        self.conv1 = nn.Conv1d(len_conv1, nr_kernels, kernel_size=kernel_size)\n",
    "        self.maxpool1 = nn.MaxPool1d(3,1)\n",
    "        if self.nr_conv_layers == 3:\n",
    "            self.conv2 = nn.Conv1d(nr_kernels, nr_kernels * 2, kernel_size=kernel_size)\n",
    "            self.maxpool2 = nn.MaxPool1d(3,1)\n",
    "            self.conv3a = nn.Conv1d(nr_kernels*2, nr_kernels, kernel_size=kernel_size)\n",
    "        elif self.nr_conv_layers == 2:\n",
    "            self.conv3b = nn.Conv1d(nr_kernels, nr_kernels, kernel_size=kernel_size)\n",
    "        else:\n",
    "            pass\n",
    "        self.maxpool3 = nn.MaxPool1d(3, 1)\n",
    "        self.len_lin3 = nr_kernels * (seq_len - self.nr_conv_layers * (kernel_size - 1) - self.nr_conv_layers * (3 - 1))\n",
    "        self.linear4 = nn.Linear(self.len_lin3, hidden_size)\n",
    "        self.linear5 = nn.Linear(hidden_size, int(hidden_size / 10))\n",
    "\n",
    "        self.linear6_mu = nn.Linear(int(hidden_size / 10), c)\n",
    "        if self.heteroscedastic:\n",
    "            self.linear6_logvar = nn.Linear(int(hidden_size / 10), 1)\n",
    "\n",
    "        # concrete dropout for convolutional layers\n",
    "        self.conc_drop1 = ConcreteDropout(dropout=dropout_conv, concrete=concrete, p_fix=p_fix,\n",
    "                                          weight_regularizer=weight_regularizer,\n",
    "                                          dropout_regularizer=dropout_regularizer, conv=self.conv, Bayes=Bayes)\n",
    "        self.conc_drop2 = ConcreteDropout(dropout=dropout_conv, concrete=concrete, p_fix=p_fix,\n",
    "                                          weight_regularizer=weight_regularizer,\n",
    "                                          dropout_regularizer=dropout_regularizer, conv=self.conv, Bayes=Bayes)\n",
    "        self.conc_drop3a = ConcreteDropout(dropout=dropout_conv, concrete=concrete, p_fix=p_fix,\n",
    "                                          weight_regularizer=weight_regularizer,\n",
    "                                          dropout_regularizer=dropout_regularizer, conv=self.conv, Bayes=Bayes)\n",
    "        self.conc_drop3b = ConcreteDropout(dropout=dropout_conv, concrete=concrete, p_fix=p_fix,\n",
    "                                          weight_regularizer=weight_regularizer,\n",
    "                                          dropout_regularizer=dropout_regularizer, conv=self.conv, Bayes=Bayes)\n",
    "        # concrete dropout for dense layers\n",
    "        self.conc_drop4 = ConcreteDropout(dropout=dropout, concrete=concrete, p_fix=p_fix,\n",
    "                                          weight_regularizer=weight_regularizer,\n",
    "                                          dropout_regularizer=dropout_regularizer, conv=\"lin\", Bayes=Bayes)\n",
    "        self.conc_drop5 = ConcreteDropout(dropout=dropout, concrete=concrete, p_fix=p_fix,\n",
    "                                          weight_regularizer=weight_regularizer,\n",
    "                                          dropout_regularizer=dropout_regularizer, conv=\"lin\", Bayes=Bayes)\n",
    "        self.conc_drop6_mu = ConcreteDropout(dropout=dropout, concrete=concrete, p_fix=p_fix,\n",
    "                                            weight_regularizer=weight_regularizer,\n",
    "                                            dropout_regularizer=dropout_regularizer, conv=\"lin\", Bayes=Bayes)\n",
    "        if self.heteroscedastic:\n",
    "            self.conc_drop6_logvar = ConcreteDropout(dropout=dropout, concrete=concrete, p_fix=p_fix,\n",
    "                                                weight_regularizer=weight_regularizer,\n",
    "                                                dropout_regularizer=dropout_regularizer, conv=\"lin\", Bayes=Bayes)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        \n",
    "    def forward(self, x_cat, x_range, stop_dropout=False):\n",
    "        '''\n",
    "        ARGUMENTS:\n",
    "        x_cat: categorical variables. Torch tensor (batch size x sequence length x number of variables)\n",
    "        x_range: range variables. Torch tensor (batch size x sequence length x number of variables)\n",
    "        stop_dropout: if \"True\" prevents dropout during inference for deterministic models\n",
    "\n",
    "        OUTPUTS:\n",
    "        mean: outputs (point estimates). Torch tensor (batch size x number of outputs)\n",
    "        log_var: log of uncertainty estimates. Torch tensor (batch size x number of outputs)\n",
    "        regularization.sum(): sum of KL regularizers over all model layers\n",
    "        '''\n",
    "\n",
    "        regularization = torch.empty(7, device=x_range.device)    #x.device = cuda:0 here\n",
    "\n",
    "        if self.no_of_embs != 0:\n",
    "            x = [emb_layer(x_cat[:, :, i])\n",
    "                 for i, emb_layer in enumerate(self.emb_layers)]\n",
    "            x = torch.cat(x, -1)\n",
    "            x = torch.cat([x, x_range], -1)\n",
    "        else:\n",
    "            x = x_range\n",
    "\n",
    "        x = torch.transpose(x, 1, 2)  # reshape from (N, seq_len, nr_features) to (N, nr_features, seq_len)\n",
    "\n",
    "        x, regularization[0] = self.conc_drop1(x, nn.Sequential(self.conv1, self.relu, self.maxpool1), stop_dropout)\n",
    "        if self.nr_conv_layers == 2:\n",
    "            x, regularization[2] = self.conc_drop3b(x, nn.Sequential(self.conv3b, self.relu, self.maxpool3), stop_dropout)\n",
    "        elif self.nr_conv_layers == 3:\n",
    "            x, regularization[1] = self.conc_drop2(x, nn.Sequential(self.conv2, self.relu, self.maxpool2), stop_dropout)\n",
    "            x, regularization[2] = self.conc_drop3a(x, nn.Sequential(self.conv3a, self.relu, self.maxpool3), stop_dropout)\n",
    "        else:\n",
    "            pass\n",
    "        x3 = x.view(-1, self.len_lin3)\n",
    "        x4, regularization[3] = self.conc_drop4(x3, nn.Sequential(self.linear4, self.relu), stop_dropout)\n",
    "        x5, regularization[4] = self.conc_drop5(x4, nn.Sequential(self.linear5, self.relu), stop_dropout)\n",
    "        mean, regularization[5] = self.conc_drop6_mu(x5, self.linear6_mu, stop_dropout)\n",
    "        if self.heteroscedastic:\n",
    "            log_var, regularization[6] = self.conc_drop6_logvar(x5, self.linear6_logvar, stop_dropout)\n",
    "        else:\n",
    "            regularization[6] = 0\n",
    "            log_var = torch.empty(mean.size())\n",
    "\n",
    "        return mean, log_var, regularization.sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
