{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-algeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Based on:\n",
    "https://github.com/yaringal/ConcreteDropout/blob/master/concrete-dropout-pytorch.ipynb\n",
    "Extensions:\n",
    "+ 1D and 2D convolutional layers\n",
    "+ possibility to use with fixed dropout parameters and deterministic models, etc.\n",
    "'''\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class ConcreteDropout(nn.Module):\n",
    "    def __init__(self, dropout=True, concrete=True, p_fix=0.01, weight_regularizer=1e-6,\n",
    "                 dropout_regularizer=1e-5, conv=\"lin\", Bayes=True):\n",
    "        super().__init__()\n",
    "\n",
    "        '''\n",
    "        ARGUMENTS:\n",
    "        dropout: in case of deterministic model, apply dropout if \"True\", otherwise no dropout\n",
    "        concrete: dropout parameter is fixed when \"False\". If \"True\", then concrete dropout\n",
    "        p_fix: dropout parameter used in case of not self.concrete\n",
    "        weight_regularizer: parameter for weight regularization in reformulated ELBO\n",
    "        dropout_regularizer: parameter for dropout regularization in reformulated ELBO\n",
    "        conv: \"lin\" for dense layers, \"1D\" or \"2D\" for 1D or 2D convolutional layers\n",
    "        Bayes: BNN if \"True\", deterministic model if \"False\" (only sampled once for inference)\n",
    "        '''\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.concrete = concrete\n",
    "        self.p_fix  = p_fix\n",
    "        self.weight_regularizer = weight_regularizer\n",
    "        self.dropout_regularizer = dropout_regularizer\n",
    "        self.conv = conv\n",
    "        self.Bayes = Bayes\n",
    "\n",
    "        self.p_logit = nn.Parameter(torch.FloatTensor([0]))\n",
    "\n",
    "\n",
    "    def forward(self, x, layer, stop_dropout=False):\n",
    "        '''\n",
    "        ARGUMENTS:\n",
    "        x: input for the (concrete) dropout layer wrapper\n",
    "        layer: layer to be called after application of dropout mask\n",
    "        stop_dropout: if \"True\" prevents dropout during inference for deterministic models\n",
    "\n",
    "        OUTPUTS:\n",
    "        out: output for the (concrete) dropout layer wrapper\n",
    "        regularization: corresponding KL term\n",
    "        '''\n",
    "\n",
    "        if self.concrete:\n",
    "            p = torch.sigmoid(self.p_logit)\n",
    "        else:\n",
    "            p = torch.tensor(self.p_fix).cuda()\n",
    "\n",
    "        if (self.dropout and not stop_dropout) or self.Bayes:\n",
    "            out = layer(self._concrete_dropout(x, p, self.concrete))\n",
    "        else:\n",
    "            out = layer(x)\n",
    "\n",
    "        sum_of_square = 0\n",
    "        for param in layer.parameters():\n",
    "            sum_of_square += torch.sum(torch.pow(param, 2))\n",
    "\n",
    "        regularization, weights_regularizer, dropout_regularizer = 0, 0, 0\n",
    "        if self.Bayes:\n",
    "            weights_regularizer = self.weight_regularizer * sum_of_square / (1 - p)\n",
    "            if self.concrete:\n",
    "                dropout_regularizer = p * torch.log(p)\n",
    "                dropout_regularizer += (1. - p) * torch.log(1. - p)\n",
    "                if self.conv == \"lin\":\n",
    "                    input_dimensionality = x[0].numel()\n",
    "                elif self.conv == \"1D\":\n",
    "                    input_dimensionality = list(x.size())[1]\n",
    "                else:\n",
    "                    input_dimensionality = list(x.size())[1]\n",
    "                dropout_regularizer *= self.dropout_regularizer * input_dimensionality\n",
    "            regularization = weights_regularizer + dropout_regularizer  # KL(q(W)|p(W))) eq. 3 in concrete dropout paper\n",
    "\n",
    "        return out, regularization\n",
    "\n",
    "\n",
    "    def _concrete_dropout(self, x, p, concrete):\n",
    "        '''\n",
    "        ARGUMENTS:\n",
    "        x: input for the (concrete) dropout layer wrapper\n",
    "        p: dropout parameter\n",
    "        concrete: dropout parameter is fixed when \"False\". If \"True\", then concrete dropout\n",
    "\n",
    "        OUTPUTS:\n",
    "        x: input after application of dropout mask\n",
    "        '''\n",
    "\n",
    "        if not concrete:\n",
    "            if self.conv == \"lin\":\n",
    "                drop_prob = torch.bernoulli(torch.ones(x.shape).cuda()*p)\n",
    "            elif self.conv == \"1D\":\n",
    "                drop_prob = torch.bernoulli(torch.ones(list(x.size())[0], list(x.size())[1], 1).cuda()*p)\n",
    "                drop_prob= drop_prob.repeat(1, 1, list(x.size())[2])\n",
    "            else:\n",
    "                drop_prob = torch.bernoulli(torch.ones(list(x.size())[0], list(x.size())[1], 1, 1).cuda()*p)\n",
    "                drop_prob = drop_prob.repeat(1, 1, list(x.size())[2], list(x.size())[3])\n",
    "\n",
    "        else:\n",
    "            eps = 1e-7         # to avoid torch.log(0)\n",
    "            temp = 0.1         # temperature\n",
    "\n",
    "            if self.conv == \"lin\":\n",
    "                unif_noise = torch.rand_like(x)\n",
    "            elif self.conv == \"1D\":\n",
    "                unif_noise = torch.rand(list(x.size())[0], list(x.size())[1], 1).cuda()\n",
    "                unif_noise = unif_noise.repeat(1, 1, list(x.size())[2])\n",
    "            else:\n",
    "                unif_noise = torch.rand(list(x.size())[0], list(x.size())[1], 1, 1).cuda()\n",
    "                unif_noise = unif_noise.repeat(1, 1, list(x.size())[2], list(x.size())[3])\n",
    "\n",
    "            drop_prob = (torch.log(p + eps)\n",
    "                         - torch.log(1 - p + eps)\n",
    "                         + torch.log(unif_noise + eps)\n",
    "                         - torch.log(1 - unif_noise + eps))\n",
    "\n",
    "            drop_prob = torch.sigmoid(drop_prob / temp)\n",
    "\n",
    "        random_tensor = 1 - drop_prob\n",
    "        retain_prob = 1 - p\n",
    "        x = torch.mul(x, random_tensor)\n",
    "        x /= retain_prob\n",
    "\n",
    "        return x\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
