{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catholic-amino",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "adapted from:\n",
    "https://github.com/nlhkh/dropout-in-rnn\n",
    "'''\n",
    "\n",
    "from typing import Optional, Tuple\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from Models_12_gh import ConcreteDropout\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class StochasticLSTMCell(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, p_fix=0.01, concrete=True,\n",
    "                 weight_regularizer=.1, dropout_regularizer=.1, Bayes=True):\n",
    "        '''\n",
    "        ARGUMENTS:\n",
    "        input_size: number of features (after embedding layer)\n",
    "        hidden_size: number of nodes in LSTM layers\n",
    "        p_fix: dropout parameter used in case of not self.concrete\n",
    "        concrete: dropout parameter is fixed when \"False\". If \"True\", then concrete dropout\n",
    "        weight_regularizer: parameter for weight regularization in reformulated ELBO\n",
    "        dropout_regularizer: parameter for dropout regularization in reformulated ELBO\n",
    "        Bayes: BNN if \"True\", deterministic model if \"False\" (only sampled once for inference)\n",
    "        '''\n",
    "\n",
    "        super(StochasticLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.concrete = concrete\n",
    "        self.wr = weight_regularizer\n",
    "        self.dr = dropout_regularizer\n",
    "        self.Bayes = Bayes\n",
    "\n",
    "        if concrete:\n",
    "            self.p_logit = nn.Parameter(torch.empty(1).normal_())\n",
    "        else:\n",
    "            if np.isnan(p_fix):\n",
    "                p_fix = .5\n",
    "            self.p_logit = torch.full([1], p_fix)\n",
    "\n",
    "        self.Wi = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.Wf = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.Wo = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.Wg = nn.Linear(self.input_size, self.hidden_size)\n",
    "\n",
    "        self.Ui = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.Uf = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.Uo = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.Ug = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        k = torch.tensor(self.hidden_size, dtype=torch.float32).reciprocal().sqrt()\n",
    "\n",
    "        self.Wi.weight.data.uniform_(-k, k).cuda()\n",
    "        self.Wi.bias.data.uniform_(-k, k).cuda()\n",
    "\n",
    "        self.Wf.weight.data.uniform_(-k, k).cuda()\n",
    "        self.Wf.bias.data.uniform_(-k, k).cuda()\n",
    "\n",
    "        self.Wo.weight.data.uniform_(-k, k).cuda()\n",
    "        self.Wo.bias.data.uniform_(-k, k).cuda()\n",
    "\n",
    "        self.Wg.weight.data.uniform_(-k, k).cuda()\n",
    "        self.Wg.bias.data.uniform_(-k, k).cuda()\n",
    "\n",
    "        self.Ui.weight.data.uniform_(-k, k).cuda()\n",
    "        self.Ui.bias.data.uniform_(-k, k).cuda()\n",
    "\n",
    "        self.Uf.weight.data.uniform_(-k, k).cuda()\n",
    "        self.Uf.bias.data.uniform_(-k, k).cuda()\n",
    "\n",
    "        self.Uo.weight.data.uniform_(-k, k).cuda()\n",
    "        self.Uo.bias.data.uniform_(-k, k).cuda()\n",
    "\n",
    "        self.Ug.weight.data.uniform_(-k, k).cuda()\n",
    "        self.Ug.bias.data.uniform_(-k, k).cuda()\n",
    "\n",
    "    def _sample_mask(self, batch_size, stop_dropout):\n",
    "        '''\n",
    "        ARGUMENTS:\n",
    "        batch_size: batch size\n",
    "        stop_dropout: if \"True\" prevents dropout during inference for deterministic models\n",
    "\n",
    "        OUTPUTS:\n",
    "        zx: dropout masks for inputs. Tensor (GATES x batch_size x input size (after embedding))\n",
    "        zh: dropout masks for hiddens states. Tensor (GATES x batch_size x number hidden states)\n",
    "        '''\n",
    "\n",
    "        if not self.concrete:\n",
    "            p = self.p_logit.cuda()\n",
    "        else:\n",
    "            p = torch.sigmoid(self.p_logit).cuda()\n",
    "        GATES = 4\n",
    "        eps = torch.tensor(1e-7)\n",
    "        t = 1e-1\n",
    "\n",
    "        if not stop_dropout:\n",
    "            ux = torch.rand(GATES, batch_size, self.input_size).cuda()\n",
    "            uh = torch.rand(GATES, batch_size, self.hidden_size).cuda()\n",
    "\n",
    "            if self.input_size == 1:\n",
    "                zx = (1 - torch.sigmoid((torch.log(eps) - torch.log(1 + eps)\n",
    "                                         + torch.log(ux + eps) - torch.log(1 - ux + eps))\n",
    "                                        / t))\n",
    "            else:\n",
    "                zx = (1 - torch.sigmoid((torch.log(p + eps) - torch.log(1 - p + eps)\n",
    "                                         + torch.log(ux + eps) - torch.log(1 - ux + eps))\n",
    "                                        / t)) / (1 - p)\n",
    "            zh = (1 - torch.sigmoid((torch.log(p + eps) - torch.log(1 - p + eps)\n",
    "                                     + torch.log(uh + eps) - torch.log(1 - uh + eps))\n",
    "                                    / t)) / (1 - p)\n",
    "        else:\n",
    "            zx = torch.ones(GATES, batch_size, self.input_size).cuda()\n",
    "            zh = torch.ones(GATES, batch_size, self.input_size).cuda()\n",
    "\n",
    "        return zx, zh\n",
    "\n",
    "    \n",
    "    def regularizer(self):\n",
    "        '''\n",
    "        OUTPUTS:\n",
    "        self.wr * weight_sum: weight regularization in reformulated ELBO\n",
    "        self.wr * bias_sum: bias regularization in reformulated ELBO\n",
    "        self.dr * dropout_reg: dropout regularization in reformulated ELBO\n",
    "        '''\n",
    "\n",
    "        if not self.concrete:\n",
    "            p = self.p_logit.cuda()\n",
    "        else:\n",
    "            p = torch.sigmoid(self.p_logit)\n",
    "\n",
    "        if self.Bayes:\n",
    "            weight_sum = torch.tensor([\n",
    "                torch.sum(params ** 2) for name, params in self.named_parameters() if name.endswith(\"weight\")\n",
    "            ]).sum() / (1. - p)\n",
    "\n",
    "            bias_sum = torch.tensor([\n",
    "                torch.sum(params ** 2) for name, params in self.named_parameters() if name.endswith(\"bias\")\n",
    "            ]).sum()\n",
    "\n",
    "            if not self.concrete:\n",
    "                dropout_reg = torch.zeros(1)\n",
    "            else:\n",
    "                dropout_reg = self.input_size * (p * torch.log(p) + (1 - p) * torch.log(1 - p))\n",
    "            return self.wr * weight_sum, self.wr * bias_sum, self.dr * dropout_reg\n",
    "        else:\n",
    "            return torch.zeros(1)\n",
    "\n",
    "\n",
    "    def forward(self, input: Tensor, stop_dropout=False) -> Tuple[\n",
    "        Tensor, Tuple[Tensor, Tensor]]:\n",
    "        '''\n",
    "        ARGUMENTS:\n",
    "        input: Tensor (sequence length x batch size x input size(after embedding) )\n",
    "        stop_dropout: if \"True\" prevents dropout during inference for deterministic models\n",
    "\n",
    "        OUTPUTS:\n",
    "        hn: tensor of hidden states h_t. Dimension (sequence_length x batch_size x hidden size)\n",
    "        h_t: hidden states at time t. Dimension (batch size x hidden size (number of nodes in LSTM layer)\n",
    "        c_t: cell states. Dimension (batch size x hidden size (number of nodes in LSTM layer)\n",
    "        '''\n",
    "\n",
    "        seq_len, batch_size = input.shape[0:2]\n",
    "\n",
    "        h_t = torch.zeros(batch_size, self.hidden_size, dtype=input.dtype).cuda()\n",
    "        c_t = torch.zeros(batch_size, self.hidden_size, dtype=input.dtype).cuda()\n",
    "\n",
    "        hn = torch.empty(seq_len, batch_size, self.hidden_size, dtype=input.dtype)\n",
    "\n",
    "        zx, zh = self._sample_mask(batch_size, stop_dropout)\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            x_i, x_f, x_o, x_g = (input[t] * zx_ for zx_ in zx)\n",
    "            h_i, h_f, h_o, h_g = (h_t * zh_ for zh_ in zh)\n",
    "\n",
    "            i = torch.sigmoid(self.Ui(h_i) + self.Wi(x_i))\n",
    "            f = torch.sigmoid(self.Uf(h_f) + self.Wf(x_f))\n",
    "            o = torch.sigmoid(self.Uo(h_o) + self.Wo(x_o))\n",
    "            g = torch.tanh(self.Ug(h_g) + self.Wg(x_g))\n",
    "\n",
    "            c_t = f * c_t + i * g\n",
    "            h_t = o * torch.tanh(c_t)\n",
    "            hn[t] = h_t\n",
    "            hn = hn.cuda()\n",
    "\n",
    "        return hn, (h_t, c_t)\n",
    "\n",
    "\n",
    "class StochasticLSTM(nn.Module):\n",
    "    \"\"\"LSTM stacked layers with dropout and MCMC\"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        '''\n",
    "        ARGUMENTS:\n",
    "        emb_dims: list of tuples (a, b) for each categorical variable,\n",
    "                  with a: number of levels, and b: embedding dimension\n",
    "        hidden_size: number of nodes in LSTM layers\n",
    "        weight_regularizer: parameter for weight regularization in reformulated ELBO\n",
    "        dropout_regularizer: parameter for dropout regularization in reformulated ELBO\n",
    "        \"input_size\": number of range variables\n",
    "        hs: \"True\" if heteroscedastic, \"False\" if homoscedastic\n",
    "        dropout: in case of deterministic model, apply dropout if \"True\", otherwise no dropout\n",
    "        concrete: dropout parameter is fixed when \"False\". If \"True\", then concrete dropout\n",
    "        p_fix: dropout parameter in case \"concrete\"=\"False\"\n",
    "        Bayes: BNN if \"True\", deterministic model if \"False\" (only sampled once for inference)\n",
    "        nr_lstm_layers: number of LSTM layers\n",
    "        '''\n",
    "\n",
    "        super(StochasticLSTM, self).__init__()\n",
    "\n",
    "        defaultKwargs = {\"emb_dims\": None, \"hidden_size\": 10, \"weight_regularizer\": .1, \"dropout_regularizer\": .1,\n",
    "                         \"input_size\": 1, \"hs\": True, \"dropout\": True, \"concrete\":True, \"p_fix\": .01,\n",
    "                         \"Bayes\": True, \"nr_lstm_layers\": 3}\n",
    "        kwargs = {**defaultKwargs, **kwargs}\n",
    "        self.emb_dims = kwargs[\"emb_dims\"]\n",
    "        self.hidden_size = kwargs[\"hidden_size\"]\n",
    "        self.weight_regularizer = kwargs[\"weight_regularizer\"]\n",
    "        self.dropout_regularizer = kwargs[\"dropout_regularizer\"]\n",
    "        self.input_size = kwargs[\"input_size\"]\n",
    "        self.heteroscedastic = kwargs[\"hs\"]\n",
    "        self.dropout = kwargs[\"dropout\"]\n",
    "        self.concrete = kwargs[\"concrete\"]\n",
    "        self.p_fix = kwargs[\"p_fix\"]\n",
    "        self.Bayes = kwargs[\"Bayes\"]\n",
    "        self.nr_layers = kwargs[\"nr_lstm_layers\"]\n",
    "\n",
    "        self.no_of_embs = 0\n",
    "        if self.emb_dims:\n",
    "            self.emb_layers = nn.ModuleList([nn.Embedding(x, y)\n",
    "                                             for x, y in self.emb_dims])\n",
    "            self.no_of_embs = sum([y for x, y in self.emb_dims])\n",
    "\n",
    "        self.input_size += self.no_of_embs\n",
    "\n",
    "        self.first_layer = StochasticLSTMCell(self.input_size, self.hidden_size, p_fix=self.p_fix, concrete=self.concrete,\n",
    "                                              weight_regularizer=self.weight_regularizer,\n",
    "                                              dropout_regularizer=self.dropout_regularizer, Bayes=self.Bayes)\n",
    "        self.hidden_layers = nn.ModuleList(\n",
    "            [StochasticLSTMCell(self.hidden_size, self.hidden_size, self.p_fix, concrete=self.concrete,\n",
    "                                weight_regularizer=self.weight_regularizer,\n",
    "                                dropout_regularizer=self.dropout_regularizer,\n",
    "                                Bayes=self.Bayes) for i in range(self.nr_layers - 1)])\n",
    "        self.linear1 = nn.Linear(self.hidden_size, 5)\n",
    "        self.linear2_mu = nn.Linear(5, 1)\n",
    "        if self.heteroscedastic:\n",
    "            self.linear2_logvar = nn.Linear(5, 1)\n",
    "\n",
    "\n",
    "        self.conc_drop1 = ConcreteDropout(dropout=self.dropout, concrete=self.concrete, p_fix=self.p_fix,\n",
    "                                          weight_regularizer=self.weight_regularizer,\n",
    "                                          dropout_regularizer=self.dropout_regularizer, conv=\"lin\", Bayes=self.Bayes)\n",
    "        self.conc_drop2_mu = ConcreteDropout(dropout=self.dropout, concrete=self.concrete, p_fix=self.p_fix,\n",
    "                                            weight_regularizer=self.weight_regularizer,\n",
    "                                            dropout_regularizer=self.dropout_regularizer, conv=\"lin\", Bayes=self.Bayes)\n",
    "        if self.heteroscedastic:\n",
    "            self.conc_drop2_logvar = ConcreteDropout(dropout=self.dropout, concrete=self.concrete, p_fix=self.p_fix,\n",
    "                                                weight_regularizer=self.weight_regularizer,\n",
    "                                                dropout_regularizer=self.dropout_regularizer, conv=\"lin\", Bayes=self.Bayes)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    def regularizer(self):\n",
    "        total_weight_reg = self.first_layer.regularizer()\n",
    "        for l in self.hidden_layers:\n",
    "            total_weight_reg += l.regularizer()\n",
    "        return total_weight_reg\n",
    "\n",
    "\n",
    "    def forward(self, x_cat, x_range, stop_dropout=False):\n",
    "        '''\n",
    "        ARGUMENTS:\n",
    "        x_cat: categorical variables. Torch tensor (batch size x sequence length x number of variables)\n",
    "        x_range: range variables. Torch tensor (batch size x sequence length x number of variables)\n",
    "        stop_dropout: if \"True\" prevents dropout during inference for deterministic models\n",
    "\n",
    "        OUTPUTS:\n",
    "        mean: outputs (point estimates). Torch tensor (batch size x number of outputs)\n",
    "        log_var: log of uncertainty estimates. Torch tensor (batch size x number of outputs)\n",
    "        regularization.sum(): sum of KL regularizers over all model layers\n",
    "\n",
    "        '''\n",
    "\n",
    "        regularization = torch.empty(4, device=x_range.device)\n",
    "\n",
    "        if self.no_of_embs != 0:\n",
    "            x = [emb_layer(x_cat[:, :, i])\n",
    "                 for i, emb_layer in enumerate(self.emb_layers)]\n",
    "            x = torch.cat(x, -1)\n",
    "            x = torch.cat([x, x_range], -1)\n",
    "        else:\n",
    "            x = x_range\n",
    "\n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        batch_size = x.shape[1]\n",
    "        h_n = torch.zeros(self.nr_layers, batch_size, self.first_layer.hidden_size)\n",
    "        c_n = torch.zeros(self.nr_layers, batch_size, self.first_layer.hidden_size)\n",
    "\n",
    "        outputs, (h, c) = self.first_layer(x)\n",
    "        h_n[0] = h\n",
    "        c_n[0] = c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
